{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background:\n",
    "Albeit hearing a little about forward & backward propagation and knowing the basic structures of Neural Network, I, as a high school student, did not have a clear mathematical or computer knowledge to fully comprehend how the neural network tunes its node values to enhance its accuracy. \n",
    "\n",
    "From yesterday's tutorial (see commmits), which is introduction to `torch.autograd`, I could conduct forward and backward prop, which I did not clearly grasp.\n",
    "\n",
    "\n",
    "So, today, I am going to learn basic concepts of forward and backward prop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation:\n",
    "1. The input data is fed into the *forward direction* through the network\n",
    "2. Each hidden layer accepts the input data\n",
    "3. Processes the data to the activation function\n",
    "4. Passes it to the successive layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preactivation:\n",
    "- Weighted sum of inputs (= linear transformation of weights)\n",
    "- Based on this *aggregated sum* & *activation function*, the neuron makes a deicison to pass or not\n",
    "\n",
    "`z1 = w1*a0 + b`:  Weight parameter multiplied, bias added\\\n",
    "`a1 = g1(z1)`:  Activation function applied\\\n",
    "Passed onto the next layer\\\n",
    "\n",
    "`z2 = w2*a1 + b`\\\n",
    "`a2 = g2(z2)`\\\n",
    "Repeated....\n",
    "\n",
    "`Wn` is the weight parameter of the hidden layer (between the I/O & applies weights to the inputs --> directs them to the activation function)\n",
    "`z` is the intermediate variable\n",
    "\n",
    "### Activation:\n",
    "The calculated weighted sum of inputs --> Activation function\\\n",
    "\n",
    "**Activation Function**:\\\n",
    "Mathematical function that *adds non-linearity to the network*\n",
    "\n",
    "**Commonly used Activation Functions**:\n",
    "1. Sigmoid\n",
    "2. Hyperbolic Tangent (tanh)\n",
    "3. ReLU\n",
    "4. Softmax\n",
    "\n",
    "\n",
    "### Activation Function vs Loss Function\n",
    "**Activation Function:**\n",
    "- Activates the neuron that is required for the desired output\n",
    "- Converts linear input to non-linear output\n",
    "\n",
    "**Loss Function:**\\\n",
    "Helps to figure out:\n",
    "- The performance of the model in prediction\n",
    "- How good the model is able to generalize\\\n",
    "\n",
    "**Computes the error for every training**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation:\n",
    "- Method of calculating the gradient of neural network parameters\n",
    "- Understanding how changing the weights and biases in a network changes the cost function\n",
    "\n",
    "\n",
    "1. Traverses the network in reverse order (from output to input layer --> using *chain rule* in calculus)\n",
    "2. Stores any intermediate variables (partial derivatives) required while calculating the gradient with respect to some parameters\n",
    "3. Compute the gradients & errors\n",
    "4. Adjust the weights and the biases of the model accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source:\n",
    "- https://d2l.ai/chapter_multilayer-perceptrons/backprop.html\n",
    "- https://towardsdatascience.com/forward-propagation-in-neural-networks-simplified-math-and-code-version-bbcfef6f9250\n",
    "- https://www.youtube.com/watch?v=a8i2eJin0lY&ab_channel=Deeplearning.ai\n",
    "- https://deepai.org/machine-learning-glossary-and-terms/hidden-layer-machine-learning#:~:text=In%20neural%20networks%2C%20a%20hidden,inputs%20entered%20into%20the%20network.\n",
    "- https://www.analyticssteps.com/blogs/how-decide-which-activation-function-and-loss-function-use#:~:text=The%20activation%20function%20activates%20the,input%20to%20non%2Dlinear%20output.&text=Loss%20function%20helps%20you%20figure,the%20error%20for%20every%20training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
